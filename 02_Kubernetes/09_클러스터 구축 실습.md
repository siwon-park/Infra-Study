![kubernets](https://user-images.githubusercontent.com/93081720/174333422-4e2f7a03-f585-4edf-884c-0af7fea7ac5d.png)

# 09_EC2 쿠버네티스 클러스터 구축 실습

직접 EC2에 쿠버네티스 클러스터를 구축해보자

※ 참조: https://dongle94.github.io/kubernetes/kubernetes-cluster-build/

<br>

## 1. 설치 및 사전 준비

### 1) EC2 환경 구성

#### (1) 마스터-워커 노드 IP 주소 확인

```bash
마스터: 172.26.8.229
워커: 172.26.0.105
```

<br>

#### (2) NTP 서버 동기화

`NTP(Network Time Protocol)`는 클러스터를 구축하면 각 노드들이 네트워크 통신을 하는데, 각 노드들의 시간이 맞지 않아 통신에 문제가 발생할 수도 있기 때문에 이러한 리스크를 없애기 위해 설정하였다. (굳이 실습만 진행할 거면 넘어가도 무방하다.)

```bash
$ sudo apt install ntp
```

- 마스터 노드 동작 확인

```bash
$ sudo service ntp reload
$ sudo ntpq -p
```

- 워커 노드 설정 변경
  - 워커 노드에도 ntp를 설치한다.
  - 단, 마스터 노드와 달리 워커 노드는 마스터 노드와 설정 동기화가 필요하다.

```bash
# 주석이 해제되어 있는 pool과 server를 주석 처리하고 마스터 노드의 IP를 추가함
$ sudo vi /etc/ntp.conf

server [마스터 노드 IP]

# 재시작 및 동기화 확인
$ sudo systemctl restart ntp
$ sudo ntpq -p
```

<br>

#### (3) 스왑 메모리 기능 off

쿠버네티스 클러스터는 스왑 메모리가 활성화 되어 있는 것을 허용하지 않는다. 마찬가지로 `kubeadm`이 스왑 메모리를 허용하지 않기 때문에 스왑 메모리를 설정한 적이 있다면 해제한다.

```bash
$ sudo swapoff -a
```

<br>

### 2) Docker 설치

[공식 문서](https://docs.docker.com/engine/install/ubuntu/) 참조

#### (1) 도커 데몬 컨테이너 런타임 변경

`cgroup`드라이버를 변경해준다. 자세한 내용은 [공식 문서](https://kubernetes.io/ko/docs/setup/production-environment/container-runtimes/)를 참조.

※ cgroup: `control group`을 의미하며, Linux에서 프로세스에 할당된 리소스를 관리, 제한하는데 사용되는 프로세스 그룹이다.

cgroup 드라이버를 변경을 해주는 이유는 기본적으로 두 개의 cgroup 드라이버(`cgroupfs`, `systemd`)가 있는데, `kubelet`의 기본 cgroup 드라이버는 `cgroupfs`인데, 리눅스 배포판의 init 시스템의 cgroup 드라이버는 `systemd`이다.

이렇게 서로 다른 두 개의 cgroup 드라이버를 사용하면, 해당 시스템은 두 개의 다른 cgroup 관리자를 갖는 것이다. 두 개의 cgroup 관리자를 사용하게 되면, 프로세스의 리소스를 관리함에 있어 혼란을 가져오기 때문에 불안정해질 수 밖에 없다.

따라서 이러한 불안정성을 없애기 위해서 cgroup 드라이버를 일치시키는 것이다.

- 도커 데몬의 컨테이너 런타임 환경을 `systemd`로 교체

```bash
$ sudo mkdir /etc/docker
$ sudo cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF
$ sudo mkdir -p /etc/systemd/system/docker.service.d
$ sudo systemctl daemon-reload
$ sudo systemctl restart docker
```

- 만약에 GPU 자원을 사용해야 하는 경우, 다음과 같이 추가 구성

```bash
$ sudo mkdir /etc/docker
$ cat <<EOF | sudo tee /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",

  "runtimes": {
        "nvidia": {
            "path": "nvidia-container-runtime",
            "runtimeArgs": []
        }
    },
  "default-runtime": "nvidia"
}
EOF
$ sudo systemctl daemon-reload
$ sudo systemctl restart docker
```

- cgroup 드라이버 변경 확인

```bash
$ sudo docker info | grep -i cgroup
```

![cgroup](https://github.com/siwon-park/Infra_Study/assets/93081720/f35d5acd-33d1-459e-9cba-fcee3706c286)

<br>

### 3) 쿠버네티스 Set Up (저장소 추가 및 Kubeadm 설치)

gpg 저장소를 추가하고, `kubeadm`을 설치할 수 있는 환경을 만들어 설치해준다. 역시 자세한 내용은 [공식문서](https://kubernetes.io/ko/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)를 참조.

#### (1) 패키지 리스트 업데이트

```bash
$ sudo apt-get update
$ sudo apt-get upgrade
```

<br>

#### (2) 데비안 기반 배포판 설치

```bash
$ sudo apt-get update
$ sudo apt-get install -y apt-transport-https ca-certificates curl
```

<br>

#### (3) 구글 클라우드 public signing key 다운로드

```bash
$ sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
```

<br>

#### (4) 쿠버네티스 apt 레포지토리 추가

```bash
echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
```

<br>

#### (5) apt 패키지 색인 업데이트 및 kubelet, kubeadm, kubectl 설치

`kubelet`, `kubeadm`, `kubectl` 설치 후 버전을 고정한다.

```bash
$ sudo apt-get update
$ sudo apt-get install -y kubelet kubeadm kubectl
$ sudo apt-mark hold kubelet kubeadm kubectl # 버전 고정
```

- 버전 확인

```bash
$ sudo kubeadm version
$ sudo kubelet --version
$ sudo kubectl version
```

<br>

### 4) 워커 노드 셋업

지금까지 진행한 것은 `마스터 노드`에 대한 것이었고, `워커 노드`에도 위와 같은 과정을 반복해준다.

워커 노드에 환경을 구성하지도 않고 쿠버네티스가 안 된다고 하면 곤란하다.

<br>

## 2. 클러스터 구축

- 마스터 노드 시작
  - 기본적으로 마스터 노드는 `kubeadm init <args>`를 통해 실행 가능하다.

※ 아직 해당 명령어를 입력하지는 말자. `Pod 네트워크`를 먼저 구성하는 것이 먼저다.

```bash
$ sudo kubeadm init <args> # agrg에는 여러 옵션을 주어서 마스터 노드를 시작할 수 있다
```

<br>

### 1) Pod 네트워크 설정

클러스터에서 Pod가 서로 통신할 수 있도록 `Pod 네트워크`라는 것을 설치, 설정해줘야 한다.

`kubeadm`을 통해서 만들어진 클러스터는 `CNI (Container Network Interface)`기반의 애드온이 필요하다.

쿠버네티스에서 기본적으로 제공해주는 `kubenet`이라는 네트워크 플러그인을 사용해도 되지만, 매우 기본적이고 간단한 기능만 제공하기 때문에, 크로스 노드 네트워킹이나 네트워크 정책과 같은 고급 기능은 구현되어 있지 않아 이런 기능들을 사용하기엔 부적합하다. 이러한 이유 때문에 `kubeadm`은 kubenet을 지원하지 않고 `CNI 기반의 네트워크`만 지원한다.

- `Calico`, `Flannel`, `Wave Net`, `Cilium` 등의 CNI 네트워크가 있고, 각 네트워크마다 차이점이 존재하므로 적절한 네트워크 플러그인을 선택하면 된다.
- 또한 각 네트워크 플러그인마다 사용하는 네트워크 대역이 다르기 때문에 이를 유의해야 한다. 변경이 가능하나 지정된 네트워크 대역 내에서만 변경할 수 있음을 유의하자.
  - `Calico`를 사용할 예정이면 `--pod-network-cidr=192.168.10.0/16`로 설정한다.
  - `Flannel`을 사용할 예정이면 `--pod-network-cidr=10.244.10.0/16` 로 설정한다.
  - `Wave Net`을 사용할 예정이면 `--pod-network-cidr=10.32.0.0/12` 로 설정한다.
  - `Cilium`을 사용할 예정이면 `--pod-network-cidr=10.128.0.0/9` 로 설정한다.

<br>

### 2) 마스터 노드 실행

사용할 Pod 네트워크를 선택했으면 `kubeadm init` 명령어를 통해 마스터 노드를 실행해보자.

※ 설정한 네트워크 플러그인의 IP는 기억을 해두자.

```bash
$ sudo kubeadm init --apiserver-advertise-address=<마스터 노드 ip 주소> --pod-network-cidr=192.168.10.0/16

# 예시(Calico 네트워크 플러그인 적용)
$ sudo kubeadm init --apiserver-advertise-address=172.26.8.229 --pod-network-cidr=192.168.10.0/16
```

- 실행에 성공하면 아래와 같은 메세지와 함께 마스터 노드가 정상적으로 실행되었음을 알 수 있다.

![kubeadm_init](https://github.com/siwon-park/Infra_Study/assets/93081720/fafb2ac8-b701-49b3-9cb6-55e95a9e692d)

<br>

### 3) config 디렉토리 생성

`kubeadm`을 실행했을 때 나오는 메세지를 그대로 붙여 넣어준다.

```bash
$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

<br>

### 4) 워커 노드 연결

마찬가지로 마스터 노드를 실행할 때 나왔던 메세지 중 마지막에 있는 부분을 복사하여 `워커 노드`에서 해당 명령어를 입력해준다.

마스터 노드와 워커 노드를 연결하는 과정이며, 토큰으로 암호화되어 연결하는 것이다.

```bash
$ sudo kubeadm join 172.26.8.229:6443 --token i53j5a.fc3jaqu22dq8l2ey --discovery-token-ca-cert-hash sha256:035a74d98cc8089d6c6ffd77318eb674f5bfa02c951b4c3b666eceb70ed961aa
```

#### (1) kubeadm join 에러 발생 시

분명히 마스터 노드에서 나온 join 토큰 값을 입력했는데, 다음과 같이 토큰 에러가 발생한다면

![kubeadm_token_error](https://github.com/siwon-park/Infra_Study/assets/93081720/ecbf4bbc-5b20-4f75-80d1-87c5d964b130)

`마스터 노드`에서 토큰 리스트를 출력해서 토큰을 삭제한 다음에, 재생성하여 연결하면 된다.

```bash
$ sudo kubeadm token list # 토큰 리스트 출력
$ sudo kubeadm token delete [토큰 코드] # 토큰 삭제
$ sudo kubeadm token create --print-join-command # 토큰 재생성 및 join 커맨드와 같이 출력
```

<br>

#### (2) 노드 간 연결 확인

마스터 노드에서 아래와 같은 명령어를 입력하여 노드가 제대로 연결되었는지 확인할 수 있다.

```bash
$ sudo kubectl get nodes
```

- 마스터 노드는 `control-plane`이라고 써 있는 것을 확인할 수 있다.

![kubectl_get_node](https://github.com/siwon-park/Infra_Study/assets/93081720/4914c343-b371-4728-8ab8-df441a29b876)

<br>

### 5) 네트워크 연결

노드 간 실시간 네트워크 통신을 할 수 있도록 네트워크를 설정, 연결해준다.

- Calico

```bash
$ curl https://raw.githubusercontent.com/projectcalico/calico/v3.25.1/manifests/calico.yaml -O

# 네트워크 대역을 Calico의 기본값인 192.168.0.0/16이 아니라, 192.168.10.0/24로 변경했을 시 설정
# sed -i -e 's?192.168.0.0/16?192.168.10.0/24?g' calico.yaml

$ kubectl apply -f calico.yaml
```

만약 Calico 네트워크의 연결 대역을 변경했다면  `sed -i -e 's?192.168.0.0/16?192.168.10.0/24?g' calico.yaml` 와 같이 설정한 calico의 pod 네트워크를 변경하고자 할 때, 위와 같은 명령어로 변경하거나 직접 파일에 들어가서 수정하면 된다.

#### (1) 네트워크 연결 확인

```bash
$ sudo kubectl get pods --namespace kube-system
```

- 정상적으로 네트워크 연결까지 되었다면 `READY` 항목에 `1/1`이라고 설정되어 있는 것을 볼 수 있다.

![network_setup_success](https://github.com/siwon-park/Infra_Study/assets/93081720/75f5b856-422d-4d57-8c25-0244bf676486)

<br>

### 6) 클러스터 구축 확인

마스터 노드에서 `sudo kubectl get nodes`을 입력했을 때, 아래 사진과 같이 클러스터로 연결된 노드들의 상태가 `Ready`로 되어 있다면, 정상적으로 클러스터가 구축된 것이다.

![k8s_cluster](https://github.com/siwon-park/Infra_Study/assets/93081720/9a51baa5-b8a4-444f-a23e-189088ba394c)

<br>

## 4. 쿠버네티스 오브젝트 실행

> 쿠버네티스 오브젝트 실행해보기

### ※ kubectl

쿠버네티스를 다루기 위해선 `kubectl`이라는 명령어를 사용하는데, 이는 마스터 노드에서만 유효한 명령어이다.

`선언적 방식`으로 쿠버네티스 오브젝트를 실행시킬 예정인데, 오브젝트들은 워커 노드에서 실행될 예정이지만 `모든 명령어는 마스터 노드에 내리고, 마스터 노드에서 워커 노드로 전달하는 방식으로 동작`한다.

<img src="https://user-images.githubusercontent.com/93081720/222951405-7f6119a8-1ed5-4bc3-84db-8258f23e8a87.png" referrerpolicy="no-referrer" alt="image" height = "500px">

여기서 말하고자 하는 바는,내가 했던 어처구니 없는 실수를 하지 말라는 의미인데, 오브젝트들이 워커 노드에서 실행되기 때문에 `yml`파일을 워커 노드에 작성하고 워커 노드에서 초기에 실행시키는 개념이라고 생각했다.

하지만 그게 아님을 유의하자.

- 워커 노드에서 `kubectl`로 yml 파일을 실행시키면 `The connection to the server localhost:8080 was refused - did you specify the right host or port?`라는 에러 메세지가 나온다.
- 워커 노드에서 `sudo kubectl config view` 명령어로 연결된 클러스터 정보를 확인하면 `null`이 나올 것이다.
- 마스터 노드에서 해당 명령어를 실행하면 연결된 클러스터 정보가 제대로 나올 것이다.

| 워커 노드                                                    | 마스터 노드                                                  |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![cluster_null](https://github.com/siwon-park/Infra_Study/assets/93081720/1086d52a-2a05-4bb5-b450-b40ed93a228d) | ![master_node_cluster](https://github.com/siwon-park/Infra_Study/assets/93081720/50c826e9-a08e-4bae-9dc6-d2ae954a958b) |

<br>

### 1) Nginx 파드 배포

`sudo kubectl apply -f [yml 파일]`

```bash
$ sudo kubectl apply -f pod-nginx.yml
```

- pod-nginx.yml

```yml
# pod-nginx.yaml

apiVersion: v1
kind: Pod
metadata:
  name: pod-nginx
spec:
  containers:
  - name: my-nginx-pod
    image: nginx:latest
    ports:
    - containerPort: 80
      protocol: TCP
```

<br>

#### (1) 실행 확인

```bash
$ sudo kubectl describe pods [파드 명]
또는
$ sudo kubectl get deploy,pods,rs # 디플로이먼트, 파드, 레플리카셋 (띄워쓰기 없음)
또는
$ sudo kubectl get all
```

다음과 같이 워커 노드에서 nginx pod가 실행 중임을 확인할 수 있다.

![nginx-pod](https://github.com/siwon-park/Infra_Study/assets/93081720/be61705b-c88d-4634-92ef-e90df8bf7d27)

<br>

#### (2) 노드 정보 확인

```bash
$ sudo kubectl get node -o wide
```

<br>

### 2) nginx 디플로이먼트 배포

> [심화1] nginx를 deployment로 배포해보자

nginx를 4개의 pod를 띄우는 레플리카를 포함한 디플로이먼트로 배포하기

- SSAFY 코치용 EC2 한계상 `Node Port`로 밖에 배포를 못해서 Node Port로 배포함

#### (1) yml 파일 작성

- deployment

```yml
# nginx-deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-nginx-deployment
spec:
  replicas: 4
  selector:
    matchLabels:
      app: my-nginx
      tier: app
  template:
    metadata:
      labels:
        app: my-nginx
        tier: app
    spec:
      containers:
        - name: my-nginx
          image: nginx:latest
          ports:
            - containerPort: 80
              protocol: TCP
```

- node-port service

```yml
# nginx-nodeport.yml
apiVersion: v1
kind: Service
metadata:
  name: nginx-nodeport
spec:
  type: NodePort
  ports:
    - port: 80
      protocol: TCP
      nodePort: 31000
  selector:
    app: my-nginx
    tier: app
```

<br>

#### (2) 오브젝트 실행

```bash
$ sudo kubectl apply -f [deployment.yml 파일]
$ sudo kubectl apply -f [service.yml 파일]
```

<br>

#### (3) 실행 확인

Node Port 방식으로 배포했으니 `[도메인 주소: 노드포트 번호]` 또는 `[퍼블릭IP: 노드포트 번호]`로 접속

![nginx-deployment](https://github.com/siwon-park/Infra_Study/assets/93081720/15a366ea-c314-4d5b-8fa8-5e58ba47bc4f)

#### ※ (번외) 실수했던 점

처음에 노드 포트로 서비스 배포 후 배포된 포트 번호로 접속해도 `curl` 명령어로는 제대로 접근이 되었으나, 웹 상에서는 접속이 불가능했다. 그래서 뭔가 잘못되었는 줄 알고 여러 시도를 하며 구글링 해보았지만 원인을 찾을 수 없었다.

알고보니 지금까지 EC2의 public ip를 잘못 알고 있었다. [private ip: 노드 포트 번호]로 접속하니 당연히 접속하지 못하는 것이었다.

사용하는 터미널 IDE에서 EC2 호스트에 접속했을 때 상단에 나오는 `IPv4 address for eth0`이 public ip인줄 알았는데, 아니었던 것이다.

제대로된 public ip 주소는 `ping [도메인 주소]`로 알 수 있다.

```bash
ping [도메인 주소]
```

![public_ip](https://github.com/siwon-park/Infra_Study/assets/93081720/5ce52aa9-1116-4b0e-b23b-87d40421e756)

<br>

## 5. 프로젝트 배포

> Springboot 백엔드 서버를 배포해보자

Springboot 백엔드 서버 이미지를 만들고 배포한 다음에 오토 힐링(파드 삭제) 테스트를 해보자

### 1) 백엔드 서버 배포

사전에 Springboot 백엔드 이미지를 만들고, 이를 Docker Hub 레포지토리에 등록해두자. 바닐라 서버여도 상관 없다.

#### (1) yml 파일 작성

- deployment

```yml
# Deployment-test
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-test
spec:
  replicas: 4
  selector:
    matchLabels:
      app: spring-kubernetes
      tier: app
  template:
    metadata:
      labels:
        app: spring-kubernetes
        tier: app
    spec:
      containers:
        - name: spring-kubernetes
          image: zow777/siwonpark:springboot-kubernetes
          ports:
          - containerPort: 8080
```

- node-port service

```yml
# spring-nodeport.yml
apiVersion: v1
kind: Service
metadata:
  name: spring-nodeport
spec:
  type: NodePort
  ports:
    - port: 8080
      protocol: TCP
      nodePort: 32000
  selector:
    app: spring-kubernetes
    tier: app
```

<br>

#### (2) 오브젝트 실행

 ```bash
 $ sudo kubectl apply -f [deployment.yml 파일]
 $ sudo kubectl apply -f [service.yml 파일]
 ```

<br>

#### (3) 실행 확인

정상적으로 실행되었다면, 마찬가지로 위에서 node-port로 접근하는 방법을 사용해서 web상으로 접속해보자.

![springboot_node-port](https://github.com/siwon-park/Infra_Study/assets/93081720/702d37d5-9e8f-4464-b351-4f197d7f2bba)

<br>

### 2) 오토 힐링 테스트

쿠버네티스의 장점 중 하나인 `오토 힐링`을 직접 확인해보자.

파드를 하나 삭제했을 때, 자동적으로 새로운 파드가 실행되는지 확인하면 된다.

#### (1) 파드 삭제

```bash
$ sudo kubectl delete pod [파드 명]
```

<br>

#### (2) 오토 힐링 확인

`kubectl get pods`명령어로 파드 이름 및 정보를 확인하여 특정 파드를 삭제해보자.

![auto-healing](https://github.com/siwon-park/Infra_Study/assets/93081720/6cf59769-bd55-469d-a123-aba85ed8c90e)

분명히 `kubectl delete pod`명령어로 `deployment-test-6cb8989f4f-jkpbt`라는 deployment의 파드를 삭제하는 명령을 전달했고, `pod "deployment-test-6cb8989f4f-jkpbt" deleted`라고 성공적으로 파드를 삭제했음을 확인할 수 있다.

수학적으로 4개에서 1개를 삭제했으면 3개가 되어야 정상이지만, kubectl get pods`명령어로 파드를 다시 확인해보면 1개를 삭제했음에도 불구하고 여전히 4개이며, 삭제되었던 파드 대신에 새로운 파드가 하나 생성되어 있는 것을 확인할 수 있다.

- 삭제된 파드와 이름도 다르고 `AGE`항목을 보면 생성된지 `4s`된 와전히 새로운 파드임을 확인할 수 있다.

이상으로 쿠버네티스의 강력한 기능 중 하나인 `오토 힐링`에 대해서 테스트 해보았다.

